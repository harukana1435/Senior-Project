model:
  base_learning_rate: 1.0e-06
  target: specvqgan.models.cond_transformer.Net2NetTransformer
  params:
    cond_stage_key: feature
    transformer_config:
      target: specvqgan.modules.transformer.mingpt.GPTFeats
      params:
        feat_embedding_config:
          target: torch.nn.Conv1d
          params:
            in_channels: 2112
            out_channels: 1024
            kernel_size: 1
            padding: 0
        GPT_config:
          vocab_size: 128
          block_size: 318
          n_layer: 24
          n_head: 16
          n_embd: 1024
    first_stage_permuter_config:
      target: specvqgan.modules.transformer.permuter.ColumnMajor
      params:
        H: 5
        W: 53
    first_stage_config:
      target: specvqgan.models.vqgan.VQModel
      params:
        ckpt_path: ./logs/2024-05-15T19-51-25_solos_codebook/checkpoints/last.ckpt
        embed_dim: 256
        n_embed: 128
        ddconfig:
          double_z: false
          z_channels: 256
          resolution: 848
          in_channels: 1
          out_ch: 1
          ch: 128
          ch_mult:
          - 1
          - 1
          - 2
          - 2
          - 4
          num_res_blocks: 2
          attn_resolutions:
          - 53
          dropout: 0.0
        lossconfig:
          target: specvqgan.modules.losses.DummyLoss
    cond_stage_config:
      target: specvqgan.modules.misc.raw_feats.RawFeatsStage
data:
  target: train.ConditionedSpectrogramDataModuleFromConfig
  params:
    batch_size: 2
    num_workers: 8
    spec_dir_path: ./data/solos/features/*/melspec_10s_22050hz/
    sample_rate: 22050
    mel_num: 80
    spec_len: 860
    spec_crop_len: 848
    random_crop: false
    rgb_feats_dir_path: ./data/solos/features/*/feature_rgb_bninception_dim1024_21.5fps/
    flow_feats_dir_path: ./data/solos/features/*/feature_flow_bninception_dim1024_21.5fps/
    keypoint_feats_dir_path: ./data/solos/features/*/feature_keypoint_dim64_21.5fps/
    feat_depth: 2112
    feat_len: 215
    feat_crop_len: 212
    feat_sampler_cfg:
      target: specvqgan.data.vas.ResampleFrames
      params:
        feat_sample_size: 53
    train:
      target: specvqgan.data.vas.VASSpecsCondOnFeatsTrain
      params:
        specs_dataset_cfg:
          split_path: ./data/solos_train.txt
        condition_dataset_cfg:
          split_path: ./data/solos_train.txt
    validation:
      target: specvqgan.data.vas.VASSpecsCondOnFeatsValidation
      params:
        specs_dataset_cfg:
          split_path: ./data/solos_valid.txt
        condition_dataset_cfg:
          split_path: ./data/solos_valid.txt
lightning:
  modelcheckpoint:
    target: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      monitor: val/loss
      mode: min
      save_last: null
  callbacks:
    image_logger:
      target: train.ImageLogger
      params:
        for_specs: true
        vocoder_cfg:
          target: train.VocoderMelGan
          params:
            ckpt_vocoder: ./vocoder/logs/vggsound/
    early_stop_callback:
      target: pytorch_lightning.callbacks.EarlyStopping
      params:
        monitor: val/loss
        mode: min
        min_delta: 0.0
        patience: 2
        verbose: true
  trainer:
    distributed_backend: ddp
    gpus: 0,1,2,3,4,5,6,7
    resume_from_checkpoint: ./logs/2024-05-20T22-25-33_solos_transformer/checkpoints/last.ckpt
sampler:
  splits:
  - train
  - validation
  batch_size: 32
  num_workers: 4
  temperature: 1.0
  sample_next_tok_from_pred_dist: true
  top_k: 100
  sampling_mode: nopix
  samples_per_video: 1
  model_logdir: ./logs/2024-05-21T10-42-32_solos_transformer
  now: 2024-05-21T14-52-14
' ': null
